{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Activity - Backpropagation\n",
    "Goal - Build the components to train an neural network to solve the XOR problem.  Train the ANN and examine the results\n",
    "\n",
    "In this exercise, we implement Goodfellow's Algorithm 6.4: Backpropagation.  Most of the code is provided\n",
    "\n",
    "Students will need to code scections defining the derivatives of the activation functions (ReLu, Sigmoid) in STEP 1, and the loss functions (Mean Squared Error, Binary Cross Entropy) in STEP 2  (the code will not work until you do these things)\n",
    "\n",
    "Near the end of the notebook is the (provided) code to build and train a model to solve XOR (STEP 3).   It is recommended to rerun the training process several times, trying out different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Student Coding for activation function derivatives\n",
    "\n",
    "Helper Functions - Activations and derivatives\n",
    "\n",
    "Students should review the activation functions, then Code the sigmoid derivative and Relu derivative in the code cell below:\n",
    "\n",
    "```def dSigmoid(x):  # derivative of sigmoid```\n",
    "\n",
    "```def dRelu(z):```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"numpy array-compatible sigmoid (logistic) function\"\"\"\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def dSigmoid(x):  # derivative of sigmoid\n",
    "    \"\"\"a function which returns derivative of sigmoid\"\"\"\n",
    "    ds = None #placeholder\n",
    "\n",
    "    # in the student code area below,\n",
    "    # compute the derivative of the sigmoid function\n",
    "    # USE NUMPY (np) to make sure this operation is array-compatible\n",
    "    # Hint:  the derivative of a sigmoid can also be written as a function of a sigmoid\n",
    "\n",
    "    #######################     STUDENT CODE      ###################################\n",
    " \n",
    "    \n",
    "    ####################     END STUDENT CODE      ##################################\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def relu(z):  # rectified linear unit activation\n",
    "    \"\"\"numpy array-compatible ReLU function\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def dRelu(z):\n",
    "    \"\"\" \n",
    "    A function which returns the derivative of Rectified Linear Unit\n",
    "\n",
    "    \"\"\"\n",
    "    dr = None #placeholder\n",
    "    \n",
    "    # in the student code area below,\n",
    "    # compute the derivative of the ReLU function\n",
    "    # USE NUMPY (np) to make sure this operation is array-compatible\n",
    "    # recall that the derivative of a ReLU is not continuous (it has 2 possible values)\n",
    "    # Challenge:  Write this without an if statement\n",
    "    # Hint:  Use logic; in python, multiplying True by a scalar value yields the scalar and\n",
    "    #        multiplying False by a scalar value yields zero\n",
    "    \n",
    "    #######################     STUDENT CODE      ###################################\n",
    "    # compute the derivative of the ReLU function\n",
    " \n",
    "    \n",
    "    ####################     END STUDENT CODE      ##################################\n",
    "\n",
    "    return dr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Student Coding:\n",
    "\n",
    "You will need to code Mean Squared Error (MSE) and Binary Cross Entropy (BCE) loss functions.  Later the loss function will be selected as part of the ANN class instatiation\n",
    "\n",
    "Note that only BCE is appropriate for binary classification, but if you switch to a regression problem you should use MSE for the loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_loss(raw_outputs, desired_targets):\n",
    "    \"\"\"computes the (scalar) loss using MSE of a set of desired targets and raw_outputs \n",
    "    desired_targets is assumed to be a matrix of shape [samples, 1]\"\"\"\n",
    "\n",
    "    # note that the loss function MSE is on the RAW activation outputs\n",
    "    #  it is not the MSE on the thresholded outputs\n",
    "\n",
    "    mse = None #placeholder - add code below to compute the MSE loss\n",
    "\n",
    "    # Complete the MSE loss computation below\n",
    "\n",
    "    #######################     STUDENT CODE      ###################################\n",
    "\n",
    "\n",
    "    #####################     END STUDENT CODE   ####################################\n",
    "\n",
    "    return mse\n",
    "\n",
    "\n",
    "def compute_bce_loss(raw_outputs, desired_targets):\n",
    "    \"\"\"computes the (scalar) loss using binary cross entopy of a set of desired targets and raw_outputs\n",
    "     desired_targets is assumed to be a matrix of shape [samples, 1]\"\"\"\n",
    "\n",
    "    # note that the loss function Binary Cross Entropy (BCE) on the RAW activation outputs\n",
    "    # ... it is NOT the BCE on the thresholded outputs\n",
    "\n",
    "    bce = None #placeholder - add code below to compute the BCE loss\n",
    "\n",
    "    # Complete the BCE loss computation below\n",
    "    #  Hint:  See https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/binary-crossentropy\n",
    "\n",
    "    #######################     STUDENT CODE      ###################################\n",
    "\n",
    "\n",
    "\n",
    "    #####################     END STUDENT CODE   ####################################\n",
    "\n",
    "    return bce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN Class definition includes the following function (near the bottom of the cell)\n",
    "\n",
    "```    def compute_loss(self, inputs, desired_targets): ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    data = []\n",
    "    layers = []  #each network can have multiple layers\n",
    "    inputWidth = 1\n",
    "    outputWidth = 1\n",
    "    loss_function = []\n",
    "\n",
    "    # LAYER CLASS\n",
    "    class Layer:\n",
    "\n",
    "        \"\"\"class defining the elements of an ANN layer\"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            self.w = []\n",
    "            self.b = []\n",
    "            self.nodecount = []\n",
    "            self.activation_fcn = []\n",
    "            self.activation_fcn_derivative = []\n",
    "            self.orderNumber = []\n",
    "            self.previous = None  # link to previous layer\n",
    "            self.next = None  # link to next layer\n",
    "\n",
    "        def set_weights(self, w, b):\n",
    "            \"\"\"set the weights and bias for the layer.  Layer weights should have dimesion: (thislayer_nodecount, previouslayer_nodecount)\n",
    "            the dimension of the bias should be (thislayer_nodecount,1)\"\"\"\n",
    "            self.w = w\n",
    "            self.b = b\n",
    "            return self\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            assert self.w != []\n",
    "            assert self.b != []\n",
    "            self.b = np.random.normal(size=(self.w.shape))  # hidden weight matrix [rows = to, columns = from]\n",
    "            self.w = np.random.normal(size=(self.b.shape))  # hidden biases (column vector)\n",
    "\n",
    "        def set_activation(self, activation_fcn):\n",
    "            \"\"\"Sets the activation function for the entire layer (each layer can have its own activation)\"\"\"\n",
    "            self.activation_fcn = activation_fcn\n",
    "            return self\n",
    "\n",
    "        def set_activation_deriv(self, activation_fcn):\n",
    "            \"\"\"Sets the activation function for the entire layer (each layer can have its own activation)\n",
    "            In python, pass the function with its name only - dont include the parentheses\"\"\"\n",
    "            if activation_fcn == sigmoid:\n",
    "                self.activation_fcn_derivative = copy.deepcopy(dSigmoid)\n",
    "            elif activation_fcn == relu:\n",
    "                self.activation_fcn_derivative = copy.deepcopy(dRelu)\n",
    "            else:\n",
    "                self.activation_fcn_derivative = None\n",
    "\n",
    "        def compute_pre_activation(self, inputs):\n",
    "            \"\"\"Computes the results of the sum of the weights by the inputs and adds the bias\"\"\"\n",
    "            net = np.dot(self.w, inputs) + self.b\n",
    "            return net\n",
    "\n",
    "        def compute_bias_gradient(self, gradient):\n",
    "            \"\"\"Computes the gradient for the bias\"\"\"\n",
    "            gb = np.mean(gradient, axis=1)[:, np.newaxis]  # no regularization\n",
    "            return gb\n",
    "\n",
    "        def compute_weight_gradient(self, inputs, gradient):\n",
    "            \"\"\"Computes the gradient for the weigts at the current input value for this layer\"\"\"\n",
    "            gw = np.dot(gradient, inputs.T)\n",
    "            return gw\n",
    "\n",
    "        def compute_activation(self, net):\n",
    "            return self.activation_fcn(net)\n",
    "\n",
    "        def compute_activation_derivative(self, net):\n",
    "            return self.activation_fcn_derivative(net)\n",
    "\n",
    "        def compute_activation_gradient(self, net, gradient):\n",
    "            \"\"\"Computes the gradient for the activation function at the current net value for this layer\"\"\"\n",
    "            ga = np.multiply(gradient, net)  # Hadamard (elementwise) product\n",
    "            return ga\n",
    "\n",
    "        def compute_forward(self, inputs):\n",
    "            \"\"\"Returns layer ouput from input (shape = [nodeCount, input]) of the weighted input plus bias\n",
    "            input shape must be [lastlayer_nodeCount, samples] or [featurecount, samplecount] \"\"\"\n",
    "            net = self.compute_pre_activation(self, inputs)\n",
    "            layer_out = self.compute_activation(net)\n",
    "            return layer_out\n",
    "\n",
    "        def compute_layer_gradients(self, inputs, net, activation, gradient):\n",
    "            \"\"\" computes the loss gradient with respect to desired output of the layer\n",
    "            a set of desired targets is assumed to be matrix of shape [nodecount, samples]: SGD will have [nodecount,1]\n",
    "            hidden_inputs is assumed to be a matrix of shape [hiddenNodeCount, samples]\n",
    "            returns \n",
    "                g_loss:  the gradient of current layer\n",
    "                g_loss_b:  the gradient of the bias (for update purposes)\n",
    "                    Note:  we dont need to pass the g_loss_w gradient of the weights, but it is computed here\n",
    "                g_loss_backprop: the gradient \n",
    "            \n",
    "            \n",
    "            This follows algorithm 6.4 for the current layer starting inside the For loop\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            # f'(a(k))\n",
    "            d_activation = self.compute_activation_derivative(net)  # derivative of activation:  shape = [NodeCount, samples]\n",
    "            \n",
    "            # g <- g * f'(a(k))\n",
    "            g_loss = self.compute_activation_gradient(d_activation, gradient)  # shape = [NodeCount, samples]  [1, _ ] for outer layer\n",
    "            \n",
    "            # Delta_b(k) J = g (Take the mean across all 4 samples (batch))\n",
    "            g_loss_b = self.compute_bias_gradient(g_loss)  # mean gradient with respect to BIAS, shape = [NodeCount, 1]\n",
    "            \n",
    "            # Delta w(k) J = g * h(k-1)\n",
    "            g_loss_w = self.compute_weight_gradient(activation, g_loss)  # [1, 3]  Hidden layer outputs after activation\n",
    "            \n",
    "            # Determine gradient for the previous layer to pass backwards\n",
    "            # g <- W(k).T * g\n",
    "            g_loss_backprop = np.dot(self.w.T, g_loss)  # gradient to propagate back, shape = [hiddenNodeCount,samples]\n",
    "            \n",
    "            return g_loss_w, g_loss_b, g_loss_backprop\n",
    "\n",
    "        def update_Layer(self, weightUpdate, biasUpdate):\n",
    "            self.w = self.w + weightUpdate\n",
    "            self.b = self.b + biasUpdate\n",
    "            \n",
    "            \n",
    "    # ANN MODEL CLASS DEFINITIONS\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.layers = []\n",
    "        self.inputWidth = 1\n",
    "        self.outputWidth = 1\n",
    "        self.loss_function = []\n",
    "\n",
    "\n",
    "    def set_input_width(self, inputWidth):\n",
    "        \"\"\"defines the input layer width for the network\"\"\"\n",
    "        self.inputWidth = inputWidth\n",
    "\n",
    "    def add_layer(self, nodecount=1, activation_fcn=relu):\n",
    "        \"\"\"adds a layer to the neural network and returns the layer\n",
    "        Defaults with 1 node using relu activation\"\"\"\n",
    "        \n",
    "        oldLayerCount = len(self.layers)\n",
    "        thislayer = ANN.Layer()\n",
    "        thislayer.orderNumber = oldLayerCount + 1\n",
    "        if oldLayerCount > 0:  # other layers have been added already\n",
    "            lastLayer = self.layers[-1]\n",
    "            thislayer.previous = lastLayer\n",
    "            lastLayer.next = thislayer\n",
    "            layerInputSize = lastLayer.w.shape[1]\n",
    "\n",
    "        else:  # this will be the first layer\n",
    "            layerInputSize = self.inputWidth\n",
    "        thislayer.w = np.zeros((layerInputSize, nodecount))\n",
    "        thislayer.b = np.zeros((1, nodecount))\n",
    "        thislayer.activation_fcn = activation_fcn\n",
    "        thislayer.set_activation_deriv(activation_fcn)\n",
    "        self.layers = self.layers + [thislayer]\n",
    "        return thislayer\n",
    "\n",
    "    def set_loss_function (self, loss_function):\n",
    "        \"\"\"Sets the loss function for the entire network \n",
    "        In python, pass the function with its name only - dont include the parentheses\"\"\"\n",
    "        self.loss_function = copy.deepcopy(loss_function)\n",
    "\n",
    "    \n",
    "    def forwardPropagation(self, inputs):\n",
    "        \"\"\"Compute forward pass of two layer network\n",
    "        inputs are assumed to be (shape=[sampleCount,featureCount])\n",
    "        returns a matrix of raw outputs with one row of output per node (shape=[sampleCount, outputNodeCount])\n",
    "        Internal matrices are shaped for efficiency to avoid internal transposes (columns hold observations/samples) \"\"\"\n",
    "\n",
    "        # inputs and outputs will be transposed for efficiency during forwardPropagation and untransposed before returning\n",
    "\n",
    "        nets = []\n",
    "        activations = []\n",
    "        layer_input = inputs.T\n",
    "\n",
    "        for lnum, layer in enumerate(self.layers):\n",
    "            # inputs = inputs + inputs\n",
    "            layer_net = layer.compute_pre_activation(layer_input)\n",
    "            nets.append(layer_net)\n",
    "\n",
    "            layer_out = layer.compute_activation(layer_net)\n",
    "            activations.append(layer_out)\n",
    "\n",
    "            layer_input = layer_out\n",
    "        raw_output = layer_out.T\n",
    "        return raw_output, inputs, nets, activations\n",
    "\n",
    "    def backPropagation(self, inputs, desiredOutputs, learningRate):\n",
    "        w_grads = []\n",
    "        b_grads = []\n",
    "        # store nets and activations for each layer\n",
    "        raw_output, _, nets, activations = self.forwardPropagation(inputs)\n",
    "        layer_desired_out = desiredOutputs\n",
    "\n",
    "        # Note: This is only part of the gradient represented by the derivative of the binary cross entropy\n",
    "        # function with respects to the inputs of the logistic function at the output layer\n",
    "        #  See https://peterroelants.github.io/posts/cross-entropy-logistic/ for details\n",
    "        layer_grad = desiredOutputs - raw_output\n",
    "\n",
    "        #  computation of full gradient handled inside the loop below\n",
    "        for lnum, layer in reversed(list(enumerate(self.layers))):\n",
    "            if lnum == 0:\n",
    "                prev_layer_output = inputs.T\n",
    "            else:\n",
    "                prev_layer_output = activations[lnum - 1]\n",
    "            if lnum == len(self.layers)-1:  #OUTPUT LAYER\n",
    "                w_grad, b_grad, loss_grad = layer.compute_layer_gradients(prev_layer_output, nets[lnum], prev_layer_output,\n",
    "                                                                          layer_grad.T)\n",
    "            else:\n",
    "                w_grad, b_grad, loss_grad = layer.compute_layer_gradients(prev_layer_output, nets[lnum], prev_layer_output,\n",
    "                                                                          layer_grad)\n",
    "            layer.update_Layer(w_grad * learningRate, b_grad * learningRate)\n",
    "            layer_grad = loss_grad\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Compute predictions using forward propagation for single binary classification at threshold\n",
    "        X is a standard dataFrame without biases (shape=[observationCount,featureCount])\n",
    "        returns a standard column vector of binary predictions in {0,1}: (shape=[observationCount, 1])\"\"\"\n",
    "        raw_predictions, net_inputs, net_lst, activation_lst = self.forwardPropagation(X)\n",
    "        preds = raw_predictions > threshold\n",
    "        return preds\n",
    "\n",
    "    def compute_loss(self, inputs, desired_targets):\n",
    "        return self.loss_function(inputs, desired_targets)\n",
    "    \n",
    "    \n",
    "    def fit(self, inputs, desired_targets, learningRate, learningRateDecay, tolerance=1e-2, max_epochs = 1000):\n",
    "        done = False\n",
    "        loss_history = []\n",
    "        print(\"Training Model...\")\n",
    "        epoch_counter = 1\n",
    "        while not done:\n",
    "            learningRate = learningRate * learningRateDecay\n",
    "            preds = self.predict(inputs)\n",
    "            correct = desired_targets == preds\n",
    "            raw_outputs = self.forwardPropagation(inputs)[0]\n",
    "            curr_loss = self.compute_loss(raw_outputs, desired_targets).item()\n",
    "            loss_history.append(curr_loss)\n",
    "            if curr_loss < tolerance:\n",
    "                done = True\n",
    "            if epoch_counter > max_epochs:\n",
    "                done = True\n",
    "            # run an epoch of backprop\n",
    "            self.backPropagation(inputs, desired_targets, learningRate=learningRate)\n",
    "            epoch_counter+=1\n",
    "        print(\"Training Complete!\")\n",
    "\n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for decision boundary based on threshold exceeding cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDecisionBoundaryBool2(model, featureData, labelData, title, threshold = 0.5):\n",
    "    '''Build decision boundary figrue for 2-input, 1-output boolean logic functions\n",
    "    Note that this assumes a hard sigmoid was used and establishes a cutoff at 0.5\n",
    "    for predicting 0 or 1'''\n",
    "    cutoff = threshold  # Use 0.5 for Sigmoid. 0.0 for TANH\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    # Create filled countour map to color both sides of the boundary\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.c_[X.ravel(), Y.ravel()]\n",
    "    preds = model.predict(grid)  # get predictions\n",
    "    z = preds.reshape(X.shape) > cutoff  # cutoff on predictions to return boolean output\n",
    "    plt.contourf(X, Y, z, cmap='YlOrBr')\n",
    "\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.5)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(txt.item(), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show2dFunctionOutput(model_function, featureData, labelData, title):\n",
    "    \"\"\"display RAW results of arbitrary model function on 2-input (x1,x2) , 1-output (z) graphs\"\"\"\n",
    "    # cutoff = 0.5  # 0.5 for Sigmoid. 0.0 for TANH\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    # Create filled countour map to color both sides of the boundary\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.c_[X.ravel(), Y.ravel()]\n",
    "    outputs, _, _, _ = model_function(grid)  # get predictions\n",
    "    z = outputs.reshape(X.shape)  # reshape predictions for 2d representation\n",
    "    plt.contourf(X, Y, z, cmap='YlOrBr')\n",
    "\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.5)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(txt.item(), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for getting Boolean logic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_data(gate='XOR'):\n",
    "    \"\"\" Two dimensional inputs for logic gates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gate : str\n",
    "        Must be one of these {'AND', 'OR', 'XOR'}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like, shape(samples, features)\n",
    "        Two dim input for logic gates\n",
    "\n",
    "    truth[gate] : array-like, shapes(samples, )\n",
    "        The truth value for this logic gate\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.array([[0., 0.],\n",
    "                  [0., 1.],\n",
    "                  [1., 0.],\n",
    "                  [1., 1.]])\n",
    "\n",
    "    truth = {\n",
    "        'AND': np.array([0, 0, 0, 1]),\n",
    "        'OR': np.array([0, 1, 1, 1]),\n",
    "        'XOR': np.array([0, 1, 1, 0])\n",
    "    }\n",
    "\n",
    "    return X, truth[gate][:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for instantiating, training and evaluating an ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ANN_model(hidden_nodes=3,\n",
    "                    learning_rate=1.0,\n",
    "                    lr_decay=0.999,\n",
    "                    hidden_act=sigmoid,\n",
    "                    output_act=sigmoid,\n",
    "                    output_loss_fcn = compute_bce_loss,\n",
    "                    gate_type='XOR',\n",
    "                    max_epochs = 1000):\n",
    "    \n",
    "    X, Y = get_input_output_data(gate=gate_type)\n",
    "\n",
    "    model = ANN()\n",
    "    model.set_input_width(X.shape[1])\n",
    "    model.set_loss_function(output_loss_fcn)\n",
    "    \n",
    "    l1_weights = np.random.rand(hidden_nodes, 2)\n",
    "    l1_bias = np.random.rand(hidden_nodes, 1)\n",
    "    l2_weights = np.random.rand(1, hidden_nodes)\n",
    "    l2_bias = np.random.rand(1, 1)\n",
    "\n",
    "    layer1 = model.add_layer(nodecount=hidden_nodes, activation_fcn=hidden_act)\n",
    "    layer1.set_weights(l1_weights, l1_bias)\n",
    "    layer2 = model.add_layer(nodecount=1, activation_fcn=output_act)\n",
    "    layer2.set_weights(l2_weights, l2_bias)\n",
    "\n",
    "    preds = model.predict(X)\n",
    "    correct = Y == preds\n",
    "\n",
    "    print()\n",
    "    print(\"predictions: \\n\", preds)\n",
    "    print(\"Predictions correct?: \\n\", correct)\n",
    "\n",
    "    loss_history = model.fit(inputs=X, desired_targets=Y, learningRate=learning_rate, learningRateDecay=lr_decay,\n",
    "                             tolerance=1e-10, max_epochs=max_epochs)\n",
    "\n",
    "    preds = model.predict(X)\n",
    "    correct = Y == preds\n",
    "    \n",
    "    print()\n",
    "    print(\"predictions: \\n\", preds)\n",
    "    print(\"Predictions correct?: \\n\", correct)\n",
    "\n",
    "    show2dFunctionOutput(model.forwardPropagation, X, Y, \"Raw Response of Network - student\")\n",
    "    if output_act == sigmoid:\n",
    "        threshold = 0.5 # 0.5 for sigmoid\n",
    "    makeDecisionBoundaryBool2(model, X, Y, \"Thresholded XOR predictions - student\", threshold) \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"Loss (BCE)\")\n",
    "    plt.title(\"Loss over iterations (epochs)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3:  Instantiate / Train / Eval model on XOR\n",
    "\n",
    "Now see if you can get it to work... note that since the weights and biases are initialized randomly you may need to run this several times.\n",
    "Also, you can try different settings to solve XOR:\n",
    "* You can try different quantities of ```hidden_nodes```; at least 2 are needed, but more nodes will likely make training more successful\n",
    "* ```learning_rate``` can be altered from ```1.0``` and be decayed using a value other than the default (```lr_decay=0.9999```).  The closer the ```lr_decay``` is to 1, the slower the decay, and setting ```lr_decay``` to ```1.0``` would mean having no decay.  If your small network is having trouble learning you may need to reduce the learning rate.   A slow learning rate might require a higher ```max_epochs```.\n",
    "* You can change the hidden layer activation to either \"```relu```\" or \"```sigmoid```, but a sigmoid hidden layer might be easier to train in a small node-count layer since relus could *die* if they output zero for every observation; if you want to try relu you might want to increase ```hidden_nodes``` count  (see https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24)\n",
    "* Keep the output activation (```output_act```) as sigmoid\n",
    "* Keep the ```output_loss_fcn``` as ```compute_bce_loss```\n",
    "\n",
    "\n",
    "Hint... if you are struggling getting this to solve XOR, try the suggestions in the markdown cell at the end of this jupyter notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ANN_model(hidden_nodes=2,\n",
    "                   learning_rate=1.0,\n",
    "                   lr_decay=0.9999,\n",
    "                   hidden_act=relu,  #relu or sigmoid\n",
    "                   output_act=sigmoid,\n",
    "                   output_loss_fcn = compute_bce_loss,\n",
    "                   gate_type='XOR',\n",
    "                   max_epochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example with Sigmoid for the hidden layer\n",
    "```\n",
    "train_ANN_model(hidden_nodes=3,\n",
    "                   learning_rate=1.0,\n",
    "                   lr_decay=0.9999,\n",
    "                   hidden_act=sigmoid,  #relu or sigmoid\n",
    "                   output_act=sigmoid,\n",
    "                   output_loss_fcn = compute_bce_loss,\n",
    "                   gate_type='XOR',\n",
    "                   max_epochs = 5000)\n",
    "```\n",
    "\n",
    "### Example with ReLU for the hidden layer.  Note the large number of hidden nodes\n",
    "```\n",
    "train_ANN_model(hidden_nodes=20,\n",
    "                   learning_rate=1.0,\n",
    "                   lr_decay=0.9999,\n",
    "                   hidden_act=relu,  #relu or sigmoid\n",
    "                   output_act=sigmoid,\n",
    "                   output_loss_fcn = compute_bce_loss,\n",
    "                   gate_type='XOR',\n",
    "                   max_epochs = 5000)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
