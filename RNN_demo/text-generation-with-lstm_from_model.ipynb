{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import keras\n",
    "keras.__version__\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.6.0\n",
      "Keras version 2.6.0\n",
      "Tensorflow devices:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n",
      "GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"Keras version\", keras.__version__)\n",
    "print(\"Tensorflow devices: \", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with LSTM\n",
    "\n",
    "The goal of this machine learning task is to learn a model which models the sequences fround in a text file and then use the learned model to generate new text in the style of the text it has learned.  It uses LSTMs as the model and some text from Nietzsche as the source to learn.\n",
    "\n",
    "This notebook contains the code samples found in Chapter 8, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "This version of the code only *generates* the text, using the trained models... it doesnt actually do the training.  If you want to try training models you should use the other version of the code which trains the models.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "[...]\n",
    "\n",
    "## Implementing character-level LSTM text generation\n",
    "\n",
    "\n",
    "Let's put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a \n",
    "language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. In this \n",
    "example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model \n",
    "we will learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather than a more generic model of the \n",
    "English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's start by downloading the corpus and converting it to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600901\n"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "\n",
    "path = tf.keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we will extract partially-overlapping sequences of length `maxlen`, one-hot encode them and pack them in a 3D Numpy array `x` of \n",
    "shape `(sequences, maxlen, unique_characters)`. Simultaneously, we prepare a array `y` containing the corresponding targets: the one-hot \n",
    "encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200281\n",
      "Unique characters: 59\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Our network is a single `LSTM` layer followed by a `Dense` classifier and softmax over all possible characters. But let us note that \n",
    "recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in \n",
    "recent times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the language model and sampling from it\n",
    "\n",
    "\n",
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "\n",
    "* 1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "* 2) Reweighting the distribution to a certain \"temperature\"\n",
    "* 3) Sampling the next character at random according to the reweighted distribution\n",
    "* 4) Adding the new character at the end of the available text\n",
    "\n",
    "This is the code we use to reweight the original probability distribution coming out of the model, \n",
    "and draw a character index from it (the \"sampling function\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = np.finfo(np.float32).eps  #minimum floating point number for numerical stability in calculations\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds+EPS) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUDENT EXPLORATION AREA\n",
    "\n",
    "Finally, this is the loop where we repeatedly generate new text. We start generating text using a range of different temperatures \n",
    "after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of \n",
    "temperature in the sampling strategy.\n",
    "\n",
    "Note that this section will require previously-trained models to load, those models should have the naming convention of \n",
    "\"textmodel_epoch_N_.h5\" where N is an integer which indicates how many epochs the models were trained for.  These model files\n",
    "should be located in the same directory as the code.\n",
    "\n",
    "### You can select the start and epochs to display by altering \n",
    "\n",
    "```epochs=[20]```\n",
    "### You can select the temperatures (level of randomness in next character selection) by altering \n",
    "\n",
    "```temperatures=[0.2, 0.5, 1.0, 1.2]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now view the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED TEXT: \n",
      "\n",
      "==============================================================================\n",
      " have viewed the saints of india who occupy an intermediate \n",
      "==============================================================================\n",
      "\n",
      " **************** VIEWING EPOCH 1  ******************\n",
      "\n",
      "------ Randomness temperature: 0.2\n",
      "the master the most of the streng and all provinical and all and all the stroke and the strenged the streffen are all been all the stroke the be the strike the being and man and all the most all all the streng and the strenged the stroking and all all the stroke the strengence of the deally and possible and all the problem and all the strengence of the self the streng and and all man and all the s\n",
      "\n",
      "\n",
      "------ Randomness temperature: 0.5\n",
      "elf-parially of the swates the strain suffect and all the be as it soul and with the religion of any sour all but understood unself and strok and ablition of much the present man and present of the streffenter must but the must\n",
      "all philosophiss and been conficion of the must as\n",
      "woman the permation of the most as a manifould the men the master than a soul and perhaps likes and his content and it is\n",
      "\n",
      "\n",
      "------ Randomness temperature: 1.0\n",
      " it injust in puiled that him who has but\n",
      "connentinc things do been of socreming, all perpritire all abim ning to all all the spiction. feed and men all alto-, our, and howly devertation of senes bratent, heast in ord vestelf homes of guals.\n",
      "\n",
      ": cattion of micbolder toated take of picce not mustifed, the menkess, procriture: in by un motions, the gendent by his people, limmen god invosent and man h\n",
      "\n",
      "\n",
      "------ Randomness temperature: 1.2\n",
      "imtal trative\" sympo de-place, ti, ow the wedupis is non authous, with heady he. as thin\n",
      "warting-cannert, he shade cnath\". its aidation wowad purlibpting ood thinginghisen\n",
      "by here cometour\n",
      "onderic of\n",
      "foecumes frwath\n",
      "the brific refained personstes degidater aids pisont, b\" a drenglaimed centionicate.--itrecasion, theid duciveenaied contaired.\n",
      "\n",
      "wn pgi=., this ono dany--or come they strocal, for oxtu\n",
      "\n",
      "\n",
      " **************** VIEWING EPOCH 20  ******************\n",
      "\n",
      "------ Randomness temperature: 0.2\n",
      "nt of the preserved and the spirit of the fact, and the senses of the preserved the sentiment of the fact that the sense of the sense of the state of the sense of the fact, the shame the state and and the subject, and self contrary that the sense of the spirit of the fact, the sense of the sentiment of the most man of the senses to the subject of the sense of the senses, the feeling and something \n",
      "\n",
      "\n",
      "------ Randomness temperature: 0.5\n",
      "with\n",
      "the good more history and the senses to the delicate the delicate one we consequently and dective conscience of the preserved, the interest of sublimes, and even to the easely become itself who still an experience, in its\n",
      "to be interested have the means of the denial many descanders and a dominate the seiturately more and shame and sympathy are a stapes, to be allow and the socially to the mo\n",
      "\n",
      "\n",
      "------ Randomness temperature: 1.0\n",
      "st chilial, can perhaps in merite\n",
      "the wholey begen bation and that peipo free spirits. the competiently in an\n",
      "erred through mind,\n",
      "when than soul ary way neishesen all the scerefices and live\n",
      "elevately it fortsnicated to expositionars, perhaps\n",
      "fortunatel beings and long--and it may ask all tor: nookefulned to him they have bornably secot the sublime, in my socratiss, and be france of man exultied b\n",
      "\n",
      "\n",
      "------ Randomness temperature: 1.2\n",
      "elongs gevers, thereto portaiin, his saints: that is an tharated, at lives is\n",
      "woman free different overthatiyest many, that betray indixct oe his stronger\n",
      "is: for the misnorms\n",
      "life-dlry something\n",
      "draid\n",
      "excettance of svawing,\n",
      "and through found, by human\n",
      "animationable\n",
      "origin claim\n",
      "fail of spirit.=--which sorts step fecto the besl and more, in a equally sprethus\" in s\n",
      "order and fain has no refined in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#set these to whatever epochs you want, as long as you have a model trained for that epoch #\n",
    "epochs = [1,20]  # if you want to check more than one epoch, place commas between to form a list\n",
    "temperatures = [0.2, 0.5, 1.0, 1.2]\n",
    "\n",
    "#define the start point in the text for generation\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "generated_text = text[start_index: start_index + maxlen]\n",
    "starting_text = generated_text\n",
    "print('SEED TEXT: \\n')\n",
    "print('==============================================================================')\n",
    "print(starting_text)\n",
    "print('==============================================================================')\n",
    "generated_text=starting_text\n",
    "\n",
    "\n",
    "for epoch in epochs:\n",
    "    print('\\n **************** VIEWING EPOCH', epoch, \" ******************\")\n",
    "    model = load_model('textmodel_epoch_'+str(epoch)+'_.h5')\n",
    "    \n",
    "    for temperature in temperatures:\n",
    "        print()\n",
    "        print('------ Randomness temperature:', temperature)\n",
    "        #sys.stdout.write(generated_text)\n",
    "\n",
    "\n",
    "        \n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in \n",
    "particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text \n",
    "becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as \n",
    "\"eterned\" or \"troveration\"). With a high temperature, the local structure starts breaking down and most words look like semi-random strings \n",
    "of characters. Without a doubt, here 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment \n",
    "with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting.\n",
    "\n",
    "Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and \n",
    "realistic than ours. But of course, don't expect to ever generate any meaningful text, other than by random chance: all we are doing is \n",
    "sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is \n",
    "a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. To \n",
    "evidence this distinction, here is a thought experiment: what if human language did a better job at compressing communications, much like \n",
    "our computers do with most of our digital communications? Then language would be no less meaningful, yet it would lack any intrinsic \n",
    "statistical structure, thus making it impossible to learn a language model like we just did.\n",
    "\n",
    "\n",
    "## Take aways\n",
    "\n",
    "* We can generate discrete sequence data by training a model to predict the next tokens(s) given previous tokens.\n",
    "* In the case of text, such a model is called a \"language model\" and could be based on either words or characters.\n",
    "* Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.\n",
    "* One way to handle this is the notion of _softmax temperature_. Always experiment with different temperatures to find the \"right\" one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
